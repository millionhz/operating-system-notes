{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Course Code: CS-370  Instructor: Dr. Hamad Alizai  Semester: FALL 2022</p>"},{"location":"#resources","title":"Resources","text":"<ul> <li>Operating Systems: Three Easy Pieces</li> <li>Operating System Notes - Aniruddha Tapas</li> </ul>"},{"location":"10-virtualization/","title":"Virtualization","text":""},{"location":"10-virtualization/#key-points","title":"Key points","text":"<ul> <li>OS provides a batch of virtual memory to all programs. This means that two programs, in their own context, can have the same address for a variable, but in reality the addresses are being translated by the OS.</li> <li>Why virtual memory?<ul> <li>Memory protection becomes easier.</li> <li>Decoupling makes management simple.</li> <li>Space sharing - running two 3GB program with only 4 GBs RAM</li> </ul> </li> </ul>"},{"location":"10-virtualization/#virtualizing-cpu","title":"Virtualizing CPU","text":"<ul> <li>CPUs are virtualized using Time Sharing.</li> <li>Time Sharing is running a process, pausing it and then running another process.</li> </ul>"},{"location":"10-virtualization/#time-sharing-vs-space-sharing","title":"Time Sharing vs Space Sharing","text":"<ul> <li> <p>Time Sharing is allowing the resource to be used for a little while by one entity, and then a little while by another, and so forth.</p> </li> <li> <p>Space Sharing, where a resource is divided (in space) among those who wish to use it. For example, disk space is naturally a spaceshared resource; once a block is assigned to a file, it is normally not assigned to another file until the user deletes the original file</p> </li> </ul>"},{"location":"10-virtualization/#program-sate","title":"Program Sate","text":"<ul> <li>Everything that a program needs and uses to execute.</li> </ul>"},{"location":"10-virtualization/#address-space","title":"Address Space","text":"<ul> <li>What a program can read or update in memory (RAM) when it is running.</li> </ul>"},{"location":"10-virtualization/#registers","title":"Registers","text":"<ul> <li>Program counter</li> <li>Stack Pointers</li> <li>Primitive Registers</li> </ul>"},{"location":"10-virtualization/#io-information","title":"I/O Information","text":"<ul> <li>File descriptor table</li> </ul>"},{"location":"10-virtualization/#context-switching-basics","title":"Context Switching (Basics)","text":"<ul> <li>Context Switching is implemented by saving and loading the program states of different programs.</li> </ul>"},{"location":"10-virtualization/#program-control-block-pcb","title":"Program Control Block (PCB)","text":"<ul> <li>A list of structs of all the relevant information (program state) about a process.</li> </ul>"},{"location":"10-virtualization/#switching","title":"Switching","text":"<ul> <li>PCBs are used for context switching. One programs PCB is saved and another programs PCB is loaded to switch the context.</li> <li>It takes time to change context, so it is not trivial to decide whether to switch the context or not.</li> </ul>"},{"location":"10-virtualization/#how-often-to-context-switch","title":"How often to context switch?","text":""},{"location":"10-virtualization/#process-states","title":"Process States","text":""},{"location":"10-virtualization/#running","title":"Running","text":"<ul> <li>A process is running on the processor.</li> </ul>"},{"location":"10-virtualization/#ready","title":"Ready","text":"<ul> <li>Process is ready to run on the processor but for some reason the IS has chosen not to run it.</li> </ul>"},{"location":"10-virtualization/#blocked","title":"Blocked","text":"<ul> <li>A process has performed some kind of operation that makes it not ready to run until some other event takes place.</li> <li>Example: Waiting for an I/O operation to complete.</li> </ul>"},{"location":"10-virtualization/#scheduling-de-scheduling","title":"Scheduling &amp; De-scheduling","text":"<ul> <li>Being moved from ready to running means the process has been scheduled.</li> <li>Being moved from running to ready means the process has been de-scheduled.</li> </ul>"},{"location":"10-virtualization/#process-creation","title":"Process Creation","text":"<ol> <li>Load a program code into memory, into the address space of the process<ul> <li>Programs initially reside on disk in executable format.</li> <li>OS perform the loading process lazily. (Loading pieces of code or data only as they are needed during program execution.)</li> </ul> </li> <li>The program\u2019s run-time stack is allocated.<ul> <li>Use the stack for local variables, function parameters, and return address.</li> <li>Initialize the stack with arguments \u2192 <code>argc</code> and the <code>argv</code> array of <code>main()</code> function.</li> </ul> </li> <li>The programs heap memory is initialized.<ul> <li>Used for explicitly requested dynamically allocated data.</li> <li>Program request such space by calling <code>malloc()</code> and free it by calling <code>free()</code>.</li> </ul> </li> <li>Initialize I/O; File descriptors \u2192 <code>STDIN</code>, <code>STDOUT</code> and <code>STDERR</code></li> <li>Start the program running at the entry point, namely <code>main()</code>.<ul> <li>The OS transfers control of the CPU to the newly-created process.</li> </ul> </li> </ol>"},{"location":"10-virtualization/#processes-in-linux","title":"Processes in Linux","text":"<ul> <li>In linux the processes are called tasks.</li> <li>The linux kernel stores the tasks in a circular doubly linked list called task list.</li> <li>Each element in the list is a process descriptor of type <code>struct task_struct</code> defined in <code>linux/sched.h</code></li> </ul>"},{"location":"10-virtualization/#process-api","title":"Process API","text":"<ul> <li><code>fork()</code></li> <li><code>exec()</code></li> <li><code>wait()</code></li> </ul>"},{"location":"10-virtualization/#how-does-exec-work","title":"How does <code>exec()</code> work?","text":"<ul> <li>If exec successfully parsers, it replaces the the current process with a new process (one passed through args) and execution continues from the main of the new program.</li> <li>The complete memory of the calling process is replaced except the file descriptors.</li> </ul>"},{"location":"10-virtualization/#resource-protection","title":"Resource Protection","text":""},{"location":"10-virtualization/#direct-execution","title":"Direct Execution","text":"<ul> <li>Just run the program directly on the CPU.</li> <li>This technique is simple and results in amazingly fast program execution, but this way the program runs with no restriction i.e. having full control of the CPU, memory and IO resources.</li> </ul>"},{"location":"10-virtualization/#problems","title":"Problems","text":"<ul> <li>Monopolize system resources - CPU</li> <li>Uncontrollable access to I/O</li> <li>Illegal memory access</li> </ul>"},{"location":"10-virtualization/#limited-direct-execution","title":"Limited Direct Execution","text":"<ul> <li>What we want is to allow a process to perform I/O and some other restricted operations, but without giving the process complete control over the system.</li> <li>A mode bit specifies a programs privileges and is used to transfer resource control in a protected way to the process.</li> </ul>"},{"location":"10-virtualization/#mode-bit","title":"Mode Bit","text":"<ul> <li>User mode \u2192 Hardware<ul> <li>Applications do not have full access to hardware resources (Can only manipulate assigned memory but nothing more)</li> </ul> </li> <li>Kernel mode \u2192 Hardware<ul> <li>Access to all the resources of the machine.</li> </ul> </li> <li>System Call is the boundary between these modes; it transfers control between user mode and kernel mode.</li> <li>Upon creation of a process, each process gets a kernel stack and a user stack.<ul> <li>The user stack is used for normal program execution.</li> <li>The kernel stack is used when the OS is serving a system call from that process.</li> </ul> </li> </ul>"},{"location":"10-virtualization/#trap","title":"<code>trap</code>","text":"<ul> <li>To execute a system call, a process must execute a special <code>trap</code> instruction with a system-call-number</li> <li>The <code>trap</code> instruction:<ul> <li>Jumps into kernel and set the bit to kernel mode</li> <li>Pushes the registers on to the process\u2019s kernel stack</li> </ul> </li> <li>OS performs the requested service in privileged mode</li> <li>OS calls a <code>return-from-trap</code> instruction<ul> <li>Return to caller process and set the bit to user mode</li> <li>Pop the register values from the process\u2019s kernel stack</li> </ul> </li> </ul>"},{"location":"10-virtualization/#how-does-trap-work","title":"How does <code>trap</code> work?","text":"<ul> <li>OS sets up a trap table at boot time in privileged mode and registers it with the hardware.</li> <li>The trap table tells the hardware knows that given a system-call number, what specific block of instruction to jump to and execute.</li> <li>A System-call number is assigned to each system call which acts like a kind of an index in the trap table.</li> <li>OS examines this number, ensures its valid, executes the corresponding code</li> <li>User can only specify the call number and does not know where to jump to - Protection of OS memory</li> </ul>"},{"location":"10-virtualization/#switching-between-processes","title":"Switching Between Processes","text":""},{"location":"10-virtualization/#cooperative-approach","title":"Cooperative Approach","text":"<ul> <li>OS trusts the processes to periodically give up CPU by:<ul> <li>making a system call</li> <li>doing something illegal (illegal memory access)</li> </ul> </li> </ul>"},{"location":"10-virtualization/#non-cooperative-approach","title":"Non Cooperative Approach","text":"<ul> <li>During the boot sequence, the OS starts a timer.</li> <li>The timer raises an interrupt every few milliseconds.</li> <li>On a timer interrupt:<ul> <li>The currently running process is halted.</li> <li>A pre-configured interrupt (trap) handler in the OS runs.</li> <li>CPU control is given up by the program.</li> </ul> </li> </ul>"},{"location":"10-virtualization/#context-switching-steps","title":"Context Switching (Steps)","text":"<ol> <li>Time interrupt fires \u2013 like the execution of a trap instruction.</li> <li>Save user register values of Process A on to its kernel stack.</li> <li>Handle the trap \u2013 switch().</li> <li>Save the kernel register values of the Process A in the PCB.</li> <li>Restore the kernel register values of the Process B from the PCB.</li> <li>Execute return-from-trap.</li> </ol>"},{"location":"10-virtualization/#scheduling-based-on-turnaround-time","title":"Scheduling (based on Turnaround Time)","text":""},{"location":"10-virtualization/#turnaround-time","title":"Turnaround Time","text":"\\[ T_{turnaround} = T_{completion} - T_{arrival} \\]"},{"location":"10-virtualization/#fairness-in-context-of-scheduling","title":"Fairness (in context of scheduling)","text":"<ul> <li>Every workload gets an equal Turnaround Time.</li> </ul>"},{"location":"10-virtualization/#fifo-first-in-first-out","title":"FIFO (First in, First Out)","text":"<ul> <li> <p>First Come, First Served</p> <p></p> </li> <li> <p>Performs poorly if the first process runs for a long time.</p> </li> </ul> <p></p>"},{"location":"10-virtualization/#sjf-shortest-job-first","title":"SJF (Shortest Job First)","text":"<ul> <li>The shortest job is processed first, then the next shortest and so on.</li> </ul> <ul> <li>Performs poorly when jobs arrive at different times, and the first and only arrived job happens to be the longs in the whole set of jobs.</li> </ul>"},{"location":"10-virtualization/#stfc-shortest-time-to-completion-first","title":"STFC (Shortest Time-to-Completion First)","text":"<ul> <li>When a new job enters the system:<ul> <li>Determine remaining time of each job</li> <li>Schedule the job which has the least time left</li> </ul> </li> </ul> <p>Note</p> <p>Techniques like STFC and SJF optimizes for turnaround time but perform poorly for response time.</p>"},{"location":"10-virtualization/#scheduling-based-on-response-time","title":"Scheduling (based on Response Time)","text":""},{"location":"10-virtualization/#response-time","title":"Response Time","text":"<ul> <li>The time from when the job arrives to the first time it is scheduled</li> </ul> \\[ T_{response} = T_{firstrun} - T_{arrival} \\]"},{"location":"10-virtualization/#rr-round-robin-scheduling","title":"RR (Round Robin) scheduling","text":"<ul> <li>Run a job for a time slice and then switch to the next job in the run queue until the jobs are finished.</li> <li>Time slice is sometimes called a scheduling quantum.</li> <li>The length of a time slice must be a multiple of the timer-interrupt period.<ul> <li>E.g. if the timer interrupt runs every 10 ms, the time slice could be 10, 20 or any multiple of 10.</li> </ul> </li> </ul> <p>Note</p> <p>RR is great for response time but performs poorly on metrics like turn around time.</p> <ul> <li>waiting time =Completion time - (Arrival time + execution time)</li> </ul>"},{"location":"10-virtualization/#context-switch-cost","title":"Context Switch Cost","text":"<ul> <li>The time it took to save and load registers</li> <li>Cold cache resulting from a new context.</li> </ul>"},{"location":"10-virtualization/#time-slice-length","title":"Time Slice Length","text":"Shorter Time Slice Longer Time Slice Better response time Worse response time The cost of context switching will dominate overall performance Amortize the cost of switching"},{"location":"10-virtualization/#managing-io-with-stfc","title":"Managing I/O (with STFC)","text":"<ul> <li>Assume that there a process that takes 50ms to run, but makes an I/O request that takes 10ms to run after every 10ms, i.e. a I/O operation after every 10ms sub-job.</li> <li>STFC will treat the 10ms sub-jobs of the process as independent processes and schedule accordingly.</li> <li>(Check book)</li> </ul>"},{"location":"10-virtualization/#multilevel-feedback-queue-mlfq","title":"Multilevel Feedback Queue (MLFQ)","text":"<ul> <li>A Scheduler that learns from the past to predict the future</li> <li>Without any knowledge of the process, MLFQ:<ol> <li>Optimizes turnaround time \u2192 Run shorter jobs first</li> <li>Minimizes Response Time</li> </ol> </li> </ul>"},{"location":"10-virtualization/#queues","title":"Queues","text":"<ul> <li>The MLFQ has a number of distinct queues, each assigned a different priority level.</li> <li>At any given time, a job that is ready to run is on a single queue.</li> <li>MLFQ uses priorities to decide which job should run at a given time - a job with higher priority (i.e., a job on a higher queue) is chosen to run.</li> <li>Of course, more than one job may be on a given queue, and thus have the same priority. In this case, we will just use round-robin scheduling among those jobs.</li> <li>Thus, we arrive at the first two basic rules for MLFQ:<ul> <li>Rule 1: If Priority(A) &gt; Priority(B), A runs (B doesn\u2019t).</li> <li>Rule 2: If Priority(A) = Priority(B), A &amp; B run in RR.</li> </ul> </li> <li>The high-priority queues \u2192 Short time slices<ul> <li>E.g., 10 or fewer milliseconds</li> </ul> </li> <li>The Low-priority queue \u2192 Longer time slices<ul> <li>E.g., 100 milliseconds</li> </ul> </li> </ul>"},{"location":"10-virtualization/#priority","title":"Priority","text":"<ul> <li>Rather than giving a fixed priority to each job, MLFQ varies the priority of a job based on its observed behavior.</li> <li>If, for example, a job repeatedly relinquishes the CPU while waiting for input from the keyboard, MLFQ will keep its priority high, as this is how an interactive process might behave.</li> <li>If, instead, a job uses the CPU intensively for long periods of time, MLFQ will reduce its priority.</li> <li>In this way, MLFQ will try to learn about processes as they run, and thus use the history of the job to predict its future behavior.</li> </ul>"},{"location":"10-virtualization/#changing-priorities","title":"Changing Priorities","text":"<ul> <li>Rules related to changing priorities:<ul> <li>Rule 3: When a job enters the system, it is placed at the highest priority (the topmost queue).</li> <li>Rule 4a: If a job uses up an entire time slice while running, its priority is reduced (i.e., it moves down one queue).</li> <li>Rule 4b: If a job gives up the CPU before the time slice is up, it stays at the same priority level.</li> </ul> </li> </ul> <p>Note</p> <p>Check Example 1 &amp; 2 from book</p> <ul> <li>MLFQ first assumes it might be a short job, thus giving the job high priority. If it actually is a short job, it will run quickly and complete; if it is not a short job, it will slowly move down the queues, and thus soon prove itself to be a long-running more batch-like process.</li> </ul>"},{"location":"10-virtualization/#priority-and-io","title":"Priority and I/O","text":"<ul> <li>As Rule 4b states above, if a process gives up the processor before using up its time slice, we keep it at the same priority level.</li> <li>If an interactive job, for example, is doing a lot of I/O (say by waiting for user input from the keyboard or mouse), it will relinquish the CPU before its time slice is complete; in such case, we don\u2019t wish to penalize the job and thus simply keep it at the same level.</li> </ul>"},{"location":"10-virtualization/#problems_1","title":"Problems","text":"<ol> <li>Starvation - if there are \u201ctoo many\u201d interactive jobs in the system, they will combine to consume all CPU time, and thus long-running jobs will never receive any CPU time.</li> <li>Gaming - A normal program can issue an I/O operation before the time slice is over thus relinquishing the CPU; doing so allows you to remain in the same queue, and thus gain a higher percentage of CPU time.</li> <li>A program may change its behavior over time; what was CPU bound may transition to a phase of interactivity.  With our current approach, such a job would be out of luck and not be treated like the other interactive jobs in the system.</li> </ol>"},{"location":"10-virtualization/#priority-boost-fixing-problem-1-3","title":"Priority Boost (Fixing Problem 1 &amp; 3)","text":"<ul> <li>Periodically boost the priority of all the jobs in system.<ul> <li>Rule 5: After some time period S, move all the jobs in the system to the topmost queue.</li> </ul> </li> <li>How often to boost the priority of jobs?</li> </ul>"},{"location":"10-virtualization/#better-accounting-fix-problem-2","title":"Better Accounting (Fix Problem 2)","text":"<ul> <li>Instead of forgetting how much of a time slice a process used at a given level, the scheduler should keep track.</li> <li>Once a process has used its allotment, it is demoted to the next priority queue. (Whether it uses the time slice in one long burst or many small ones does not matter.)</li> <li>We thus rewrite Rules 4a and 4b to the following single rule:<ul> <li>Rule 4: Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is reduced (i.e., it moves down one queue).</li> </ul> </li> <li>Allotment Size?</li> </ul>"},{"location":"10-virtualization/#mlfq-summary","title":"MLFQ Summary","text":""},{"location":"10-virtualization/#proportional-share-lottery-scheduling","title":"Proportional-Share - Lottery Scheduling","text":"<ul> <li>Guarantee that each job obtain a certain percentage of CPU time</li> <li>Not optimized for turnaround or response time.</li> </ul>"},{"location":"10-virtualization/#tickets","title":"Tickets","text":"<ul> <li>Represent the share of a resource that a process should receive</li> <li>The percent of tickets represents its share of the system resource in question</li> <li>Example: There are two processes, A and B<ul> <li>Process A has 75 tickets \u2192 receive 75% of the CPU</li> <li>Process B has 25 tickets \u2192 receive 25% of the CPU</li> </ul> </li> <li>Winning Ticket: One of the total distributed tickets is the winning ticker</li> <li>The scheduler holds a lottery (every so often), picks the process with the winning ticket. Then loads the state of the winning process and runs it.</li> <li>In the example below A holds ticket 0-75 and B holds 76-100.</li> </ul> <ul> <li>The longer these two jobs compete, the more likely they are to achieve the desired percentages.</li> </ul>"},{"location":"10-virtualization/#implementation","title":"Implementation","text":"<ul> <li>We need<ol> <li>Number of tickets</li> <li>Random number generator</li> <li>Linked list to track process and tickets</li> </ol> </li> </ul>"},{"location":"10-virtualization/#fairness","title":"Fairness","text":"<ul> <li>F: fairness metric - The time the first job completes divided by the time that the second job completes.</li> </ul>"},{"location":"10-virtualization/#problem","title":"Problem","text":"<ul> <li>Assigning tickets is not trivial.</li> <li>Lottery Algorithm is not deterministic.</li> </ul>"},{"location":"10-virtualization/#stride-scheduling","title":"Stride Scheduling","text":""},{"location":"10-virtualization/#stride","title":"Stride","text":"<ul> <li>A value assigned to each job</li> <li>Inversely proportional to the number of tickets</li> </ul>"},{"location":"10-virtualization/#example","title":"Example","text":"<ul> <li>Processes A (100 tix), B (50 tix), C (250 tix)</li> <li>Use a large number, say 10,000 to calculate stride</li> <li>Divide 10,000/ no of tickets, to calculate the stride of each process (A will have 100, B will have 200, C will have 40)</li> <li>Every time a process runs, its pass is increment by its stride</li> <li>Run the process that has the lowest pass value</li> </ul> <p>Note</p> <p>The results of stride scheduling are the same as the results of Lottery Scheduling. The only difference is that stride scheduling is deterministic.</p>"},{"location":"10-virtualization/#why-lottery-when-stride","title":"Why Lottery when Stride?","text":"<ul> <li>Lottery Scheduling does not have any global state.</li> <li>Imagine a new job enters in the middle of our stride scheduling example above; what should its pass value be? Should it be set to 0? If so, it will monopolize the CPU.</li> <li>With lottery scheduling, there is no global state per process; we simply add a new process with whatever tickets it has, update the single global variable to track how many total tickets we have, and go from there.</li> <li>In this way, lottery makes it much easier to incorporate new processes in a sensible manner.</li> </ul>"},{"location":"10-virtualization/#linux-completely-fair-scheduler-cfs","title":"Linux Completely Fair Scheduler (CFS)","text":"<ul> <li>Spend very little time in making scheduling decisions.</li> <li>Non-fixed timeslice.</li> <li>Enables control over priority by using nice value</li> <li>Employs efficient data structure for efficient search, insertion and deletion of a process.</li> </ul>"},{"location":"10-virtualization/#virtual-runtime-vruntime","title":"Virtual Runtime (<code>vruntime</code>)","text":"<ul> <li>Track how long each process has been executing for in a process specific variable</li> <li>Increase the variable in proportion with physical (real) time when a process runs</li> <li>When a scheduling decision occurs, CFS will pick the process with the lowest <code>vruntime</code> to run next.</li> </ul>"},{"location":"10-virtualization/#when-to-switch","title":"When to switch?","text":"<ul> <li>Often switches \u2192 More fairness</li> <li>Less switches \u2192 More performance</li> <li><code>sched_latency</code> = 48ms \u2192 timeslice = sched_latency/n</li> <li><code>min_granularity</code> = 6ms \u2192 min timeslice (for when there are too many processes)</li> </ul> <p>Note</p> <p>Check Book Page 8</p>"},{"location":"10-virtualization/#weighting-niceness","title":"Weighting (Niceness)","text":"<ul> <li>The nice parameter defines priority.</li> <li>IT can be set anywhere from -20 to +19 for a process, with a default of 0</li> <li> <ul> <li>: lower priority</li> </ul> </li> <li> <ul> <li>: higher priority</li> </ul> </li> <li>CFS maps nice values to weights, which are used to calculate time slice.</li> </ul> <ul> <li>weight0 is the weight of the default priority (0).</li> <li> <p>here runtime is the actual time that the job ran; vruntime is the scaled amount.</p> <p></p> <p></p> </li> </ul>"},{"location":"10-virtualization/#data-structures-efficiency","title":"Data Structures - Efficiency","text":"<ul> <li>Point: When the scheduler has to find the next job to run, it should do so as quickly as possible.</li> <li>For this the OS uses Red-Black Trees which are:<ul> <li>Balanced binary tree (keep depth low)</li> <li>Operations take O(log n) (efficient)</li> </ul> </li> <li>Sleep and I/O bound resources are handled differently to avoid monopolization.</li> </ul>"},{"location":"10-virtualization/#multiprogramming-and-time-sharing","title":"Multiprogramming and Time Sharing","text":"<ul> <li>Load multiple processes in memory</li> <li>Execute each of them for a short while</li> <li>Switch between them to increase utilization and efficiency</li> </ul>"},{"location":"10-virtualization/#problem_1","title":"Problem","text":"<ul> <li>Memory Protection: Errant memory access from other processes.</li> </ul>"},{"location":"10-virtualization/#address-spaces","title":"Address Spaces","text":"<ul> <li>An abstraction of physical memory</li> <li>A program\u2019s view of memory in the system</li> <li>Code<ul> <li>Where instructions live</li> </ul> </li> <li>Heap<ul> <li>Dynamically allocate memory</li> </ul> </li> <li> <p>Stack</p> <ul> <li>Store return addresses and values</li> <li>local variables and arguments to routines</li> </ul> <p></p> </li> </ul>"},{"location":"10-virtualization/#virtual-address-space","title":"Virtual Address Space","text":"<ul> <li>Every program is given a virtual address space which it thinks of as the actual memory but in reality OS is just mapping the virtual address of a program to a physical space in memory.</li> <li>A program is not aware of the fact that its memory is being virtualized</li> <li>Some goals of Virtual Address Space are:<ol> <li>Transparency</li> <li>Efficiency</li> <li>Protection</li> </ol> </li> </ul>"},{"location":"10-virtualization/#components-of-virtual-address-space","title":"Components of Virtual Address Space","text":"<ul> <li>Data: For defined global variables</li> <li>BSS: For undefined global variables</li> </ul>"},{"location":"10-virtualization/#address-translation","title":"Address Translation","text":"<ul> <li>Hardware transforms a virtual address to a physical address.</li> <li>Memory virtualizing takes a similar strategy known as limited direct execution (LDE) for efficiency and control.</li> </ul>"},{"location":"10-virtualization/#dynamic-reallocation-base-bound","title":"Dynamic Reallocation - (Base &amp; Bound)","text":"\\[ physical \\ address = virtual \\ address + base \\] \\[ 0 \\leq virtual \\ address &lt; bound \\]"},{"location":"10-virtualization/#storing-bound-register-2-ways","title":"Storing Bound Register (2 ways)","text":""},{"location":"10-virtualization/#context-switching","title":"Context Switching","text":"<p>Note</p> <p>See summary of chapter from book</p>"},{"location":"10-virtualization/#problems_2","title":"Problems","text":"<ul> <li>Internal Fragmentation<ul> <li>Big chunk of free space between heap and stack</li> <li>Wastage of memory</li> </ul> </li> <li>Can not run a program when address space does not fit into physical memory</li> </ul>"},{"location":"10-virtualization/#segmentation","title":"Segmentation","text":"<ul> <li>A segment is a contiguous portion of the address space of a particular length.</li> <li>Segmentation allows the OS to do is to place each one of those segments in different parts of physical memory, and thus avoid filling physical memory with unused virtual address space.</li> <li>Now the instead of storing 1 base bound pair, OS stores 3 for:<ul> <li>Code</li> <li>Stack</li> <li>Heap</li> </ul> </li> </ul>"},{"location":"10-virtualization/#translation-code","title":"Translation (code)","text":"\\[ physical \\ address = offset + base \\] <ul> <li>Example:</li> </ul>"},{"location":"10-virtualization/#translation-heap","title":"Translation (heap)","text":"<p>Note</p> <p>In context of memory K = 1024</p>"},{"location":"10-virtualization/#which-segment-are-we-referring-to","title":"Which Segment Are We Referring To?","text":""},{"location":"10-virtualization/#stack-and-segments","title":"Stack and Segments","text":"<ul> <li>Stack grows backwards (towards smaller addresses)</li> <li>Extra hardware support is needed to store that information</li> </ul> <p>Note</p> <p>The Max Seg Size - Offset is Ratta</p>"},{"location":"10-virtualization/#support-for-sharing","title":"Support for Sharing","text":"<ul> <li>Sometimes it is useful to share certain memory segments between address spaces.</li> <li>Hardware support is needed in the form of protection bits to indicate permissions of read, write and execute.</li> <li>In addition to checking whether a virtual address is within bounds, the hardware also has to check whether a particular access is permissible</li> </ul>"},{"location":"10-virtualization/#fine-grained-vs-coarse-grained","title":"Fine-grained vs Coarse-grained","text":"Coarse Fine small number of segments allows more flexibility for address space code, heap, stack hardware support with a segment table is required"},{"location":"10-virtualization/#fragmentation","title":"Fragmentation","text":"<ul> <li>With variable segment size, physical memory quickly becomes full of little holes of free space</li> <li>External Fragmentation: little holes of free space in physical memory that is too small for allocating segment<ul> <li>There is 24KB free, but not in one contiguous segment</li> <li>The OS cannot satisfy the 20KB request</li> </ul> </li> </ul>"},{"location":"10-virtualization/#free-space-in-context-of-user-managed-memory","title":"Free Space (in context of user-managed memory)","text":"<ul> <li>Chunks of unallocated heap memory.</li> <li>OS manages memory using segmentation while user\u2019s heap is managed by a user-level memory allocation libraries (<code>malloc()</code> and <code>free()</code> in <code>libc</code>).</li> <li>If the free space gets chopped into many little variable sized pieces (after multiple <code>malloc()</code> calls), subsequent requests might fail because there is no single contiguous space that can satisfy the request.</li> </ul> <p>Note</p> <p>Internal fragmentation can also occur if the allocator hands out a larger size of memory than the requested size.</p>"},{"location":"10-virtualization/#splitting-and-coalescing","title":"Splitting and Coalescing","text":""},{"location":"10-virtualization/#splitting","title":"Splitting","text":"<ul> <li>Assume we have a request for just a single byte of memory. In this case, the allocator will perform an action known as splitting:<ul> <li>it will find a free chunk of memory that can satisfy the request and split it into two.</li> <li>the first chunk it will return to the caller; the second chunk will remain on the list.</li> </ul> </li> </ul>"},{"location":"10-virtualization/#coalescing","title":"Coalescing","text":"<ul> <li> <p>Given this (tiny) heap, what happens when an application calls free(10), thus returning the space in the middle of the heap?</p> <ul> <li>If we simply add this free space back into our list without too much thinking, we might end up with a list that looks like this:</li> </ul> <p></p> </li> <li> <p>While the entire heap is now free, it is seemingly divided into three chunks of 10 bytes each. Thus, if a user requests 20 bytes, a simple list traversal will not find such a free chunk, and return failure.</p> </li> <li>To solve the issue we can coalesce by following these steps:<ul> <li>look carefully at the addresses of the chunk you are returning as well as the nearby chunks of free space</li> <li>if the newly freed space sits right next to one (or two, as in this example) existing free chunks, merge them into a single larger free chunk</li> </ul> </li> </ul> <p></p>"},{"location":"10-virtualization/#how-does-free-know-how-much-to-free","title":"How does <code>free()</code> know how much to free?","text":"<ul> <li>The information about the allocation size is kept in a header block, which is kept just before the handed-out chunk of memory.</li> </ul> <ul> <li>the <code>magic</code> number is for integrity checks.</li> <li>on a call to <code>free()</code> we can check the header and figure out the size:</li> </ul> <pre><code>void free(void *ptr) {\n header_t *hptr = (header_t *) ptr - 1;\n ...\n assert(hptr-&gt;magic==1234567);\n ...\n}\n</code></pre> <ul> <li>Because we are storing the header in the handed-out chunk, we have to keep the size of the header in account:</li> </ul> \\[ malloc(N) = N + \\text{size of header} \\]"},{"location":"10-virtualization/#embedding-free-list","title":"Embedding free list","text":"<ul> <li>In the library you can\u2019t call malloc to get more space.</li> <li>You have to embed the \u201cfree list\u201d within the available space.</li> <li>(See Slides and Book)</li> </ul>"},{"location":"10-virtualization/#ran-out-of-heap","title":"Ran out of heap","text":"<ul> <li>Return <code>NULL</code>.</li> <li>Call <code>sbrk()</code> to grow heap.</li> </ul>"},{"location":"10-virtualization/#basic-strategies","title":"Basic Strategies","text":"<ul> <li>Finding the most optimal free chunk.</li> </ul>"},{"location":"10-virtualization/#best-fit","title":"Best fit","text":"<ul> <li>Search through the free list and find chunks of free memory that are as big or bigger than the requested size.</li> <li>Return the one that is the smallest in that group of candidates.</li> <li>Advantage: Returns the most optimal chunk</li> <li>Drawback: Searching through the list is slow.</li> </ul>"},{"location":"10-virtualization/#worst-fit","title":"Worst fit","text":"<ul> <li>Find the largest chunk and return the requested amount.</li> <li>Keep the remaining (large) chunk on the free list.</li> <li>Advantage: Freeling space will create larger chunks through coalescing.</li> <li>Drawback: A complete search of the list is still required and thus its slow</li> </ul>"},{"location":"10-virtualization/#first-fit","title":"First Fit","text":"<ul> <li>Finds the first block that is big enough and returns the requested amount to the user.</li> <li>Advantage: Search on average is fast.</li> <li>Drawback: Pollutes the beginning of the free list with small objects.</li> </ul>"},{"location":"10-virtualization/#advanced-strategies","title":"Advanced Strategies","text":""},{"location":"10-virtualization/#segregates-lists","title":"Segregates Lists","text":"<ul> <li>Maintain multiple free lists for tracking free spaces.</li> <li>One for popular-size requests:<ul> <li>Advantages:</li> <li>No exhaustive search needed for popular-size requests</li> <li>No fragmentation for</li> </ul> </li> <li>One for general allocation</li> </ul>"},{"location":"10-virtualization/#buddy-allocation","title":"Buddy Allocation","text":"<ul> <li>Designed to simplify coalescing</li> </ul>"},{"location":"10-virtualization/#slicing","title":"Slicing","text":"<ul> <li>Free memory is first conceptually thought of as one big space of size \\(2^N\\).</li> <li>When a request for memory is made, the search for free space recursively divides free space by two until a block that is big enough to accommodate the request is found.</li> <li>The found chunk of memory is returned to the user.</li> </ul>"},{"location":"10-virtualization/#coalescing_1","title":"Coalescing","text":"<ul> <li>When returning the 8KB block to the free list, the allocator checks whether the \u201cbuddy\u201d 8KB is free. If so, it coalesces the two blocks into a 16KB block.</li> <li>The allocator then checks if the buddy of the 16KB block is still free; if so, it coalesces those two blocks.</li> </ul>"},{"location":"10-virtualization/#problems_3","title":"Problems","text":"<ul> <li>Internal Fragmentation because we can only divide memory into chucks of \\(2^x\\).</li> </ul> <p>Note</p> <p>It is very simple to determine the buddy of a particular block. The buddies only differ by a single bit.</p>"},{"location":"10-virtualization/#paging","title":"Paging","text":""},{"location":"10-virtualization/#review-base-bound","title":"Review: Base Bound","text":"<ul> <li>Give each program a fixed sized memory block.</li> <li>Big chunk of free space in the middle \u2192 needlessly consume memory</li> <li>Internal Fragmentation</li> <li>FIXED SIZE SOLUTION</li> </ul>"},{"location":"10-virtualization/#review-segmentation","title":"Review: Segmentation","text":"<ul> <li>Divide the address space in logical segments (code, stack, heap) of the memory and separately allocate memory to each segment.</li> <li>We will need three sets of base-bound registers.</li> <li>Allows more fine grained control over allocation.</li> <li>External Fragmentation</li> <li>VARIABLE SIZE SOLUTION</li> </ul>"},{"location":"10-virtualization/#paging-intro","title":"Paging - INTRO","text":"<ul> <li>Page \u2192 a fixed-sized memory unit in virtual memory</li> <li>Page Frame \u2192 a fixed-sized memory unit in physical memory</li> <li>We view physical memory as an array of fixed-sized slots called page frames; each of these frames can contain a single virtual-memory page.</li> </ul>"},{"location":"10-virtualization/#problem-paging-will-solve","title":"Problem Paging will solve","text":"<ul> <li>Flexibility: the system will be able to support the abstraction of an address space effectively, regardless of how a process uses the address space; we won\u2019t, for example, make assumptions about the direction the heap and stack grow and how they are used.</li> <li>Simplicity: It becomes very easy to manage free space (using a freelist of free pages perhaps)</li> <li>The virtual address space will stay contiguous but its mapping can be scattered in the physical memory. Assigning pages is easier as pages are fixed size and its easy to find free pages.</li> </ul>"},{"location":"10-virtualization/#page-table","title":"Page Table","text":"<ul> <li>Address Translation is performed using per-process page table.</li> </ul> <ul> <li>Virtual Address Space \u2192 the virtual memory for our program will also be divided into (contiguous) Pages. These pages will exist in the physical memory as Page Frames.</li> </ul> <ul> <li>We can go from Pages to Page frames using the Address table above:</li> </ul> <ul> <li>Page 0 is in Page Frame 3; Page 1 is in Page Frame 7;</li> </ul>"},{"location":"10-virtualization/#address-translation_1","title":"Address Translation","text":"<ul> <li>PDF: Page 3</li> </ul>"},{"location":"10-virtualization/#vpn-and-offset","title":"VPN and Offset","text":"<ul> <li>VPN = Virtual Page Number; PFN = Physical Frame Number</li> <li>We need to know:<ul> <li>Size of total VIRTUAL address space \u2192 Size of address</li> <li>address space 64 bytes, 6 bites ( 2^6 = 64)</li> <li>Size of Page \u2192 Size of offset</li> <li>if page is 16 bytes, offset will be 4 bit</li> <li>Number of Pages in VIRTUAL address space \u2192 Size of VPN</li> <li>if 4 pages, VPN is 2 bit</li> </ul> </li> </ul> <ul> <li>The MSB bits becomes the VPN.</li> <li>VPN tells us which page to address and offset tells us which specific byte in the page we want to address.</li> <li>We can use the Page table to convert VPN to PFN and thus address the physical memory.</li> </ul>"},{"location":"10-virtualization/#page-tables-are-large","title":"Page Tables are large","text":"<ul> <li>Page Tables can become larger than Segment Tables (Segmentation) &amp; Base/Bound Pairs so they need to be stored in the memory themselves.</li> <li>Page tables have to store every one to one mapping for all pages of a process.</li> </ul>"},{"location":"10-virtualization/#contents-of-page-tables-pte","title":"Contents of Page Tables &amp; PTE","text":"<ul> <li>Linear Page Table<ul> <li>Data Structure: Array</li> <li>Indexed via VPN</li> <li>Each index stores page-table entry (PTE)</li> </ul> </li> <li>PTE:<ul> <li>Valid Bit: Indicating whether the translation is valid</li> <li>Protection Bit: Indicating whether the page could be read from, written to, or executed from</li> <li>Present Bit: Indicating whether this page is in physical memory or on disk (swapped out)</li> <li>Dirty Bit: Indicating whether the page has been modified since it was brought into memory</li> <li>Accessed/Reference Bit: Indicating that a page has been accessed (useful in determining which pages are popular and thus should be kept in memory)</li> </ul> </li> </ul> <p>Note</p> <p>Valid Bit is sometimes not used. Present Bit tells the OS if a page is present or not. If a page is present than its also valid. If a page is not present of the OS needs to then validate the Page using extra Data Structures and then load if valid.</p> <p></p>"},{"location":"10-virtualization/#page-tables-are-slow","title":"Page Tables are Slow","text":"<ul> <li>For every memory reference, paging requires the hardware to perform one extra memory reference.<ul> <li>One memory reference to get the Page Frame Number</li> <li>One memory reference to access the physical memory to get the data</li> </ul> </li> <li>Memory access operations are costly.</li> <li>(Check Memory Trace from Book)</li> </ul> <p>Note</p> <p>Allocation on First Touch: Physical memory is only allocated once a process access the corresponding virtual memory for the first time  Unused pages Reclaimed: The pages may me swapped into disc if not used for long time and then must be reclaimed</p>"},{"location":"10-virtualization/#tlbtranslation-lookaside-buffer-faster-translations","title":"TLB(translation-lookaside buffer) - Faster Translations","text":"<ul> <li>Part of the systems memory-management unit (MMU)</li> <li>It is simply a hardware cache.</li> <li>Upon each virtual memory reference, the hardware first checks the TLB to see if the desired  translation is held therein ; if so, the translation is performed (quickly) without having to consult the page table (which has all translations)</li> </ul> <p>The algorithm the hardware follows works like this: first, extract the virtual page number (VPN) from the virtual address (Line 1 in Figure 19.1), and check if the TLB holds the translation for this VPN (Line 2). If it does, we have a TLB hit, which means the TLB holds the translation. Success! We can now extract the page frame number (PFN) from the relevant TLB entry, concatenate that onto the offset from the original virtual address, and form the desired physical address (PA), and access memory (Lines 5\u20137), assuming protection checks do not fail (Line 4).</p> <p>If the CPU does not find the translation in the TLB (a TLB miss), we have some more work to do. In this example, the hardware accesses the page table to find the translation (Lines 11\u201312), and, assuming that the virtual memory reference generated by the process is valid and accessible (Lines 13, 15), updates the TLB with the translation (Line 18). These set of actions are costly, primarily because of the extra memory reference needed to access the page table (Line 12). Finally, once the TLB is updated, the hardware retries the instruction; this time, the translation is found in the TLB, and the memory reference is processed quickly.</p> <ul> <li>TLB improves translation times by:<ul> <li>Spatial Locality (accessing elements of an array)</li> <li>Temporal Locality (accessing variable which was recently accessed</li> </ul> </li> </ul> <p>Note</p> <p>Larger Page Size, less number of misses.</p>"},{"location":"10-virtualization/#who-handles-the-tlb-miss","title":"Who Handles The TLB Miss?","text":""},{"location":"10-virtualization/#cisc-hardware-managed-tlb-tables","title":"CISC &amp; Hardware Managed TLB Tables","text":"<ul> <li>Hardware handles the TLB misses.</li> <li>Hardware needs to know:<ul> <li>Where the page table is (page table base register)</li> <li>Exact format of the page table</li> </ul> </li> <li>On a miss, the hardware would \u201cwalk\u201d the page table, find the correct page table entry, extract the information, update the TLB and retry the instruction.</li> </ul>"},{"location":"10-virtualization/#risc-software-managed-tlb-tables","title":"RISC &amp; Software Managed TLB Tables","text":"<ul> <li>Software (OS code in kernal mode) handles the TLB misses.</li> <li>On a TLB miss, hardware only raises an exception.</li> <li>The exception triggers the trap handler, which then executes the code to manage the TLB miss by accessing the page table, extracting the correct page table entry and using \u201cprivileged\u201d instructions to update TLB.</li> <li>After the trap completes, the hardware retries the TLB access.</li> <li>Important Points:<ul> <li>The <code>return-from-trap</code> instruction, in this case, will not resume after the instruction which caused the trap to trigger but will resume from the instruction that originally triggered the trap. (this special behavior is done by changing the PC (Program Counter) to the appropriate value)</li> <li>The TLB miss-handling code itself must not cause more TLB misses, i.e. the code must not access a page whose translation is not already present in the TLB.</li> </ul> </li> <li>Pros of Software Managed TLB:<ul> <li>Flexibility: OS can use any data structure</li> <li>Simplicity</li> </ul> </li> </ul>"},{"location":"10-virtualization/#tlb-contents","title":"TLB Contents","text":"<ul> <li>A typical TLB might have 32, 64, or 128 entries.</li> <li>Fully-Associative \u2192 any given translation can be anywhere on the TLB and hardware will search the entire TLB in parallel to find the desired translation.</li> <li>A TLB entry might look like this:</li> </ul>"},{"location":"10-virtualization/#tlb-context-switching","title":"TLB &amp; Context Switching","text":"<ul> <li>The TLB contains virtual-to-physical translations that are only valid for the currently running process; these translations are not meaningful for other processes.</li> <li>When switching from one process to another, the hardware or OS (or both) must be careful to ensure that the about-to-be-run process does not accidentally use translations from some previously run process.</li> <li>(See Lecture Slides for Visualization)</li> </ul>"},{"location":"10-virtualization/#flush-tlb","title":"Flush TLB","text":"<ul> <li>Flush the TLB on context switches.</li> <li>Can be done by switching all Valid bits to 0.</li> <li>Flaws:<ul> <li>TLB incurs misses on context switch as the whole TLB is flushed clean</li> </ul> </li> </ul>"},{"location":"10-virtualization/#asid-address-space-identifier","title":"ASID (address space identifier)","text":"<ul> <li>You can think of the ASID as a process identifier (PID), but usually it has fewer bits<ul> <li>e.g., 8 bits for the ASID versus 32 bits for a PID</li> </ul> </li> </ul> <ul> <li>With address-space identifiers, the TLB can hold translations from different processes at the same time without any confusion.</li> </ul> <p>Note</p> <p>Pages can be shared! Sharing of code pages (in binaries or shared libraries) is useful as it reduces the number of physical pages in use, thus reducing memory overhead.</p> <p>Note</p> <p>Valid bit in Page Table specifies if a certain page is assigned to a process or not. Valid bit in TLB specifies if a certain valid translation exists</p>"},{"location":"10-virtualization/#tlb-replacement-policy","title":"TLB &amp; Replacement Policy","text":"<ul> <li>when we are installing a new entry in the TLB, we have to replace an old one.</li> </ul>"},{"location":"10-virtualization/#lru-least-recently-used","title":"LRU (least recently used)","text":"<ul> <li>Evict an entry that has not recently been used.</li> <li>Take advantage of locality in the memory</li> </ul>"},{"location":"10-virtualization/#random-policy","title":"Random Policy","text":"<ul> <li>Evict a random entry to make space for a new</li> <li>Performs better in cases where a program access n+1 pages with a TLB size of n pages. In this case LRU will miss upon every access.</li> </ul>"},{"location":"10-virtualization/#tlb-entry","title":"TLB Entry","text":"<ul> <li>32-bit address space with 4KB pages:</li> </ul>"},{"location":"10-virtualization/#smaller-tables","title":"Smaller Tables","text":"<ul> <li>Linear Page Tables take up too much space.</li> <li>Example:<ul> <li>32-bit address space \u2192 2^32 addressable bytes</li> <li>4KB pages \u2192 num pages = 2^32/4*1024 = 2^20 pages</li> <li>4 byte per PFN \u2192 2^20 * 4 = 4MB of space needed</li> </ul> </li> </ul>"},{"location":"10-virtualization/#strawman-bigger-pages","title":"Strawman: Bigger Pages","text":"<ul> <li>As size of Page Table is a function of Page Size, we can increase the page size to reduce the size of page tables.</li> <li>With larger page sizes \u2192 lesser pages will be needed to store all the memory \u2192 lesser PTEs will be needed to index those pages \u2192 lower memory usage.</li> <li>Problem \u2192 wastage of memory within each page \u2192 Internal Fragmentation</li> </ul>"},{"location":"10-virtualization/#hybrid-approach-paging-segmentation-intuition","title":"Hybrid Approach: Paging + Segmentation (Intuition)","text":"<ul> <li>We dont really always use all the virtual memory space (Pages). Thus there are a lot of unused and empty pages.</li> </ul> <ul> <li>With this in mind, we can segment the pages themselves, thus eliminating those unused page entries</li> <li>Why not have one page per logical segment per process?</li> </ul>"},{"location":"10-virtualization/#segmentation-walkthrough","title":"Segmentation Walkthrough","text":"<ul> <li>Each process has three-page tables, one for each logical segment</li> <li>Base register contains the physical address of a linear page table for that segment</li> <li>Bound register indicates the end of the page table \u2192 number of pages</li> </ul> <p>Note</p> <p>Base[SN] gives us the starting address of the page table storing the segment SN</p> <ul> <li>Unallocated pages between the stack and the heap no longer take up space in a page table</li> <li>Cons:<ul> <li>Segmentation is not flexible as it assumes a certain usage pattern of the address space.</li> <li>if we have a large but sparsely-used heap, we can still end up with a lot of page table waste</li> <li>this hybrid approach causes external fragmentation to arise again; Page tables themselves will be of variable size and thus will complicate things when stored.</li> </ul> </li> </ul>"},{"location":"10-virtualization/#multi-level-page-tables","title":"Multi-level Page Tables","text":"<ul> <li>Removes unused or invalid page entries + does not require segmentation.</li> <li>Turns the Linear Page Table to a Tree.</li> <li>Theory:<ul> <li>Chop up the page table into page-sized units.</li> <li>if an entire page of page-table entries (PTEs) is invalid (not used), don\u2019t allocate that page of the page table at all.</li> <li>To track whether a page of the page table is valid (and if valid, where it is in memory), use a new structure, called the page directory.</li> </ul> </li> </ul>"},{"location":"10-virtualization/#page-directory","title":"Page Directory","text":"<ul> <li>The page directory, in a simple two-level table, contains one entry per page of the page table.</li> <li>It consists of a number of page directory entries (PDE). A PDE (minimally) has a valid bit and a page frame number (PFN), similar to a PTE.</li> <li>If the PDE is valid, it means that at least one of the pages of the page table that the entry points to (via the PFN) is valid, i.e., in at least one PTE on that page pointed to by this PDE, the valid bit in that PTE is set to one. If the PDE is not valid (i.e., equal to zero), the rest of the PDE is not defined.</li> <li>Pros:<ul> <li>Only allocated pages for the address space we are using.</li> <li>Supports sparse address spaces</li> <li>If carefully constructed, each portion of the page table fits neatly within a page, making it easier to manage memory</li> </ul> </li> <li>Cons:<ul> <li>On TLB miss, two loads from memory are required; one for the PDE and one for the PTE</li> <li>Complexity</li> </ul> </li> </ul>"},{"location":"10-virtualization/#example_1","title":"Example","text":"<ul> <li>PDF - Page 7</li> </ul>"},{"location":"10-virtualization/#more-than-2-levels","title":"More than 2 levels","text":"<ul> <li>If the Page Directory Table gets too big, it too can be stored inside another Page Directory Table.</li> <li>(Read walkthrough from the Book)</li> </ul>"},{"location":"10-virtualization/#inverted-page-tables","title":"Inverted Page Tables","text":"<ul> <li>Instead of having many page tables (one per process of the system), we keep a single page table that has an entry for each physical page of the system.</li> <li>The entry tells us which process is using this page, and which virtual page of that process maps to this physical page.</li> <li>Finding the correct entry is now a matter of searching through this data structure (which can be sped up using hash tables)</li> </ul>"},{"location":"10-virtualization/#swapping","title":"Swapping","text":"<ul> <li>Use part of disk as memory.</li> <li>Physical memories are usually much smaller than the virtual address space</li> <li>OS need a place to stash away portions of address space that currently aren\u2019t in great demand</li> </ul>"},{"location":"10-virtualization/#swap-space","title":"Swap Space","text":"<ul> <li>A reserved space on the hard disk for moving pages back and forth.</li> <li>The size of swap space defines the upper limit on the number of pages in the system</li> </ul>"},{"location":"10-virtualization/#the-present-bit-tlb","title":"The Present Bit - TLB","text":"<ul> <li>Tells the TLB if the page is present in memory or on disk.</li> <li>The act of accessing a page that is not in physical memory is commonly referred to as a page fault.</li> <li>Upon a page fault, the OS is invoked to service the page fault. A particular piece of code, known as a page-fault handler, runs, and must service the page fault.</li> <li>Page Faults are handled by OS in both hardware and software TLBs.</li> </ul>"},{"location":"10-virtualization/#page-fault-handler","title":"Page Fault Handler","text":"<ul> <li>If a page is not present in memory, a page fault handler is executed by the OS.</li> <li>The Handler looks at the PFN bits of the page, which in case of swapping, store the disk address.</li> <li>When the disk I/O complete, the OS will update the page table to mark the page as present. It will also update the PFN field to record the in-memory location of the newly-fetched page, and retry the instruction.</li> <li>(The OS can also update the TLB along with the swap of the page)</li> </ul> <p>Note</p> <p>While the I/O is in flight, the process will be in the blocked state. Thus, the OS will be free to run other ready processes.</p>"},{"location":"10-virtualization/#when-to-preform-replacement","title":"When to preform replacement?","text":""},{"location":"10-virtualization/#lazy-approach","title":"Lazy Approach","text":"<ul> <li>OS waits until memory is entirely full, and only then replaces a page to make room for some other page</li> </ul>"},{"location":"10-virtualization/#proactive-approach","title":"Proactive Approach","text":"<ul> <li>Activate Page Daemon when there are fewer than LW (low watermark) free pages.</li> <li>Evicts pages until there are HW (high watermark) pages available.</li> <li>Clustering can improve performance: cluster a number of pages and write them out at once to the swap partition.</li> </ul>"},{"location":"10-virtualization/#page-replacement-policy","title":"Page Replacement Policy","text":"<ul> <li>Our goal in picking a replacement policy for this cache is to minimize the number of cache misses (to minimize the number of times that we have to fetch a page from disk).</li> <li>Average Memory Access Time (AMAT)</li> </ul> \\[ AMAT = (T_M) + (P_{Miss} * T_D) \\]"},{"location":"10-virtualization/#optimal-swapping-policy","title":"Optimal Swapping Policy","text":"<ul> <li>Lead to the fewest number of misses overall.</li> <li>Replaces the page that will be accessed furthest in the future.</li> </ul>"},{"location":"10-virtualization/#fifo","title":"FIFO","text":"<ul> <li>Pages placed in a queue when they enter the system.</li> <li>When a replacement occurs, the page on the tail of the queue is evicted.</li> <li>Cons: Has no notion of importance of a page.</li> </ul>"},{"location":"10-virtualization/#random","title":"Random","text":"<ul> <li>Picks a random page to replace under memory pressure.</li> </ul>"},{"location":"10-virtualization/#lru-least-recently-used_1","title":"LRU - Least Recently Used","text":"<ul> <li>The more recently a page has been accessed, the more likely it will be accessed again.</li> <li>Evict the Least Recently Accessed Page.</li> </ul>"},{"location":"10-virtualization/#lfu-least-frequently-used","title":"LFU - Least Frequently Used","text":"<ul> <li>Evict the least frequently used page.</li> </ul>"},{"location":"10-virtualization/#workload-examples-notes","title":"Workload Examples - Notes","text":"<ul> <li>All approaches will do as better as random for no-locality workloads.</li> <li>LRU performs the best in 80-20, followed by FIFO and RAND that perform the same.</li> </ul>"},{"location":"10-virtualization/#approximating-lru-use-bit","title":"Approximating LRU &amp; Use bit","text":"<ul> <li>Whenever a page is referenced, the use bit is set by hardware to 1.</li> <li>This Use bit can be used by OS to determine which page to evict.</li> </ul>"},{"location":"10-virtualization/#clock-algorithm","title":"Clock Algorithm","text":"<ul> <li>Imagine all the pages of the system arranged in a circular list.</li> <li>A clock hand points to some particular page to begin with (it doesn\u2019t really matter which). When a replacement must occur, the OS checks if the currently-pointed to page P has a use bit of 1 or 0.</li> <li>If 1, this implies that page P was recently used and thus is not a good candidate for replacement.</li> <li>Thus, the use bit for P is set to 0 (cleared), and the clock hand is incremented to the next page (P + 1).</li> <li>The algorithm continues until it finds a use bit that is set to 0, implying this page has not been recently used</li> </ul>"},{"location":"10-virtualization/#thrashing","title":"Thrashing","text":"<ul> <li>If a process fills up the whole memory, the OS will always be paging. This condition is called Thrashing.</li> <li>(See Book)</li> </ul>"},{"location":"20-persistence/","title":"Persistence","text":""},{"location":"20-persistence/#io","title":"I/O","text":""},{"location":"20-persistence/#classical-system-architecture","title":"Classical System Architecture","text":"<ul> <li>Why hierarchical structure?<ul> <li>Faster Busses needs to short and are expensive to make.</li> <li>Components of high demand needs to closer to the CPU</li> </ul> </li> </ul>"},{"location":"20-persistence/#modern-system-architecture","title":"Modern System Architecture","text":"<ul> <li>Modern systems increasingly use specialized chipsets and faster point-to-point interconnects to improve performance</li> <li>CPU connects directly to high performance devices like Graphics and Memory</li> <li>I/O chips manages other basic devices by offering a number of different interconnects and forward the connection to the CPU using a standardized DMI bus.</li> </ul>"},{"location":"20-persistence/#canonical-device-protocol","title":"Canonical Device &amp; Protocol","text":"<ul> <li>A Canonical Device has:<ul> <li>Interface: Allows the system software (OS) to communicate with the device.</li> <li>Internals: Implements the abstractions the device presents to the system.</li> </ul> </li> </ul>"},{"location":"20-persistence/#interfacecontroller","title":"Interface/Controller","text":"<ul> <li>Status: Reports the current status of the device (BUSY or not)</li> <li>Command: **Stores the commands that are sent by the OS and run by the device</li> <li>Data: Used to pass and get data to the device.</li> </ul>"},{"location":"20-persistence/#protocol","title":"Protocol","text":"<ul> <li>The OS waits until the device is ready to receive command by repeatedly reading the status register (pooling).</li> <li>The OS writes data to the data register.</li> <li>The OS writes command to the command register which implicitly lets the device know that the data is present and it should start execution.</li> <li>The OS again waits until the device is ready to receive command by repeatedly reading the status register (pooling).</li> </ul> <p>Note</p> <p>Pooling is CPU intensive. The CPU wastes cycles checking the status register of the device.</p>"},{"location":"20-persistence/#pooling-and-interrupts","title":"Pooling and Interrupts","text":"<ul> <li>Instead of polling the device repeatedly, the OS can issue a request, put the calling process to sleep, and context switch to another task.</li> <li>When the device is finally finished with the operation, it will raise a hardware interrupt, causing the CPU to jump into the OS\u2019s interrupt handler.</li> <li>The interrupt handler will wake the process waiting for the I/O, which can then proceed as desired.</li> <li>Interrupts thus allow for overlap of computation and I/O, which is key for improved utilization.</li> </ul>"},{"location":"20-persistence/#problem-with-interrupts","title":"Problem with Interrupts","text":"<ul> <li>The I/O device performs the request very quickly:<ul> <li>Ideally the first poll will find the device done with the task</li> <li>but with interrupts the process (issuing IO) will have to wait for the next interrupt</li> <li>For fast IO devices, use pooling; For devices whose speed is not knows, use hybrid approach; otherwise use interrupts.</li> </ul> </li> <li>Network related I/O.<ul> <li>Livestock: When OS is doing nothing but processing interrupts.</li> <li>Such conditions arise when programs like webservers suddenly receive a large number of traffic.</li> <li>In case of interrupts, the OS will keep processing the interrupts instead of processing requests (requests are usually processed by other processes)</li> </ul> </li> <li>Coalescing: Lower the overhead of interrupts by coalescing multiple interrupts into a single interrupt delivery unit,<ul> <li>Device delivering the interrupt waits for a while, this lets other requests to be completed whose interrupts can be coalesced together.</li> </ul> </li> </ul>"},{"location":"20-persistence/#direct-memory-access-dma","title":"Direct Memory Access - DMA","text":"<ul> <li>While transferring large chunks of data to the device, the CPU has to waste a lot of time to populate the data register of the device. This trivial task wastes time that can be spent on running other processes.</li> </ul> <ul> <li>DMA is a specific device that can be programed to transfer data from memory to the CPU.</li> <li>OS programs the DMA engine by telling it where the data lives in memory, how much data to copy, and which device to send it to and DMA does the trivial task of copying by itself.</li> <li>After the transfer is complete, DMA raises an interrupt.</li> </ul>"},{"location":"20-persistence/#interacting-with-device","title":"Interacting with Device","text":""},{"location":"20-persistence/#explicit-io","title":"Explicit I/O","text":"<ul> <li>Privileged <code>in</code> and <code>out</code> instructions.</li> </ul>"},{"location":"20-persistence/#memory-mapped-io","title":"Memory-mapped I/O","text":"<ul> <li>Device registers are available as memory locations, where the OS can write data the to.</li> <li>The data then gets routed to the device.</li> <li>Advantage: Can use the <code>load</code> and <code>store</code> instructions to interact with the device (no need for special instructions)</li> </ul>"},{"location":"20-persistence/#device-drivers","title":"Device Drivers","text":"<ul> <li>Provide abstractions for device access management and control.</li> <li>Provided by the manufacturer of the device.</li> <li>Makes up majority of the code in an OS.</li> <li>Part of the kernel.</li> </ul>"},{"location":"20-persistence/#problem-with-standardized-drivers","title":"Problem with Standardized Drivers","text":"<ul> <li>Specialized functionalities provided by the device can not be utilized without updating the underlying driver.</li> </ul>"},{"location":"20-persistence/#types-of-devices","title":"Types of Devices","text":""},{"location":"20-persistence/#hard-disk-drives","title":"Hard Disk Drives","text":"<ul> <li>Consists of many sectors (512-byte blocks), each of which can be read or written</li> <li>Sectors are numbered from 0 to n \u2212 1 on a disk with n sectors.<ul> <li>Sectors have addresses.</li> </ul> </li> <li>Multi-sector operations are possible; many file systems will read or write 4KB at a time.</li> <li>The only guarantee drive manufacturers make is that a single 512-byte write is atomic.</li> <li>Accessing two blocks near one-another within the drive\u2019s address space will be faster than accessing two blocks that are far apart (not random access).</li> <li>Accessing blocks in a contiguous chunk (i.e., a sequential read or write) is the fastest access mode, and usually much faster than any more random-access pattern.</li> </ul>"},{"location":"20-persistence/#geometry","title":"Geometry","text":"<ul> <li>Platter: a circular hard surface on which data is stored persistently by inducing magnetic changes to it.</li> <li>Surface: Each platter has 2 sides, each of which is called a surface.</li> <li>Spindle: Binds together multiple platters, and connected to a motor that spins the platters around at a constant rate<ul> <li>The rate of rotation is often measured in rotations per minute (RPM), and typical modern values are in the 7,200 RPM to 15,000 RPM range</li> </ul> </li> <li>Data is encoded on each surface in concentric circles of sectors; we call one such concentric circle a track.</li> <li>A single surface contains many thousands and thousands of tracks, tightly packed together, with hundreds of tracks fitting into the width of a human hair.</li> <li>This process of reading and writing is accomplished by the disk head, one such head per surface of the drive.</li> <li>The disk head is attached to a single disk arm, which moves across the surface to position the head over the desired track.</li> </ul>"},{"location":"20-persistence/#rotational-delay","title":"Rotational Delay","text":"<ul> <li>The time it takes for the disk to rotate completely.</li> </ul>"},{"location":"20-persistence/#seek-time","title":"Seek Time","text":"<ul> <li>Time it takes to switch sectors.</li> <li>One of the costliest operations.</li> </ul>"},{"location":"20-persistence/#phases","title":"Phases","text":"<ul> <li>Acceleration - the disk arm starts to move.</li> <li>Coasting</li> <li>Deceleration</li> <li>Settling - the arms position itself onto the track</li> </ul>"},{"location":"20-persistence/#track-skew","title":"Track Skew","text":"<ul> <li>Sequential reads must be properly serviced even when crossing track boundaries.</li> <li>When switching tracks the head needs some time to settle down. This time delay is adjusted by skewing the tracks by a few sectors.</li> </ul>"},{"location":"20-persistence/#multizoning","title":"Multizoning","text":"<ul> <li>Outer tracks are bigger and can accommodate more sectors.</li> <li>Disk is organized into multiple zones: a zone is consecutive set of tracks on a surface with the same number of sectors per track.</li> <li>Outer zones have more sectors than inner zones.</li> </ul>"},{"location":"20-persistence/#track-buffer","title":"Track Buffer","text":"<ul> <li>Small amount of memory that acts as cache.</li> <li>For example, when reading a sector from the disk, the drive might decide to read in all the sectors on that track and cache them in its memory. (spatial locality)</li> <li>The OS reads and writes to this buffer.</li> </ul>"},{"location":"20-persistence/#write-back-and-write-through","title":"Write Back and Write Through","text":"<ul> <li>Write Back<ul> <li>Acknowledge that write has completed after the disk has put the data in its memory.</li> <li>This appears faster but can cause problems if the order of writes is important.</li> </ul> </li> <li>Write Through<ul> <li>Acknowledge the write after the data has been written to the platter.</li> </ul> </li> </ul>"},{"location":"20-persistence/#math","title":"Math","text":"\\[ \\text{I/O Time} = T_{seek} + T_{rotation} + T_{transfer} \\] \\[ R_{I/O} = \\frac{Size_{transfer}}{T_{I/O}} \\] <p>Note</p> <p>To calculate average rotation time, RPMs are divided by 2 as in average the disk will rotate only half way.</p>"},{"location":"20-persistence/#scheduling","title":"Scheduling","text":""},{"location":"20-persistence/#shortest-seek-time-first-sstf","title":"Shortest Seek Time First - SSTF","text":""},{"location":"20-persistence/#scan","title":"SCAN","text":"<ul> <li>The middle tracks gets served twice on average.</li> </ul>"},{"location":"20-persistence/#c-scan","title":"C-SCAN","text":"<ul> <li>Instead of sweeping in both directions across the disk, the algorithm only sweeps from outer-to-inner, and then resets at the outer track to begin again</li> <li>Doing so is a bit more fair to inner and outer tracks.</li> </ul>"},{"location":"20-persistence/#file-system","title":"File System","text":"<ul> <li>A disk is divided into fixed sized portions called blocks (each of size 4KB)</li> </ul> <ul> <li>Most of the space in the file system is users data</li> </ul> <ul> <li>File system must track information about each file<ul> <li>Which data blocks (in the data region) comprise a file, the size of the file, its owner and access rights, access and modify times, and others \u2192 inode</li> <li>inode table: reserved space on disk for inodes</li> <li>5 of 64 blocks for inodes</li> <li>Typical inode size: 128-256 bytes</li> </ul> </li> <li>Number of inodes represent the total number of files we can have on our filesystem.</li> </ul> <ul> <li>File system also needs to keep track of the unallocated or free blocks using bitmaps<ul> <li>Data bitmap: for the data region</li> <li>inode bitmap: for the inode table</li> <li>Each bit indicates whether corresponding object/block is free (0) or in-use (1)</li> <li>We use an entire 4-KB block for each of these bitmaps for simplicity</li> </ul> </li> </ul> <ul> <li>The superblock contains information about this particular file system, including, for example<ul> <li>how many inodes and data blocks are in the file system (80 and 56)</li> <li>where the inode table begins (block 3), and so forth.</li> <li>magic number to identify the file system</li> </ul> </li> <li>while mounting the file, the is reads the superblock to initialize the parameters</li> </ul>"},{"location":"20-persistence/#inode","title":"Inode","text":"<ul> <li>Each inode is implicitly referred to by a number (called the i-number)</li> <li>Given an i-number, you should directly be able to calculate where on the disk the corresponding inode is located<ul> <li>Calculate the offset into the inode region (32 x sizeof(inode) (256 bytes) = 8192</li> <li>Add start address of the inode table(12 KB) + inode region(8 KB) = 20 KB</li> </ul> </li> </ul> <pre><code>blk = (inumber * sizeof(inode_t)) / blockSize;\nsector = ((blk * blockSize) + inodeStartAddr) / sectorSize;\n</code></pre> <ul> <li>Direct Pointers: Inodes also have one or more direct pointers that point to the disk block containing the data.</li> </ul>"},{"location":"20-persistence/#multi-level-index","title":"Multi-Level Index","text":"<ul> <li>Indirect pointers are used to support files that can not be referenced by the limited number of direct pointers</li> <li>Instead of pointing to a block that contains user data, it points to a block that contains more pointers, each of which point to user data.</li> <li>To support even larger files double indirect or triple indirect pointers can be used.</li> <li>Multi-level indexes are imbalanced trees:<ul> <li>this does not effect performance as majority of the files are small and do not utilize the indirect pointers.</li> </ul> </li> </ul>"},{"location":"20-persistence/#directories","title":"Directories","text":"<ul> <li>a directory basically just contains a list of (entry name, inode number) pairs.</li> <li>file system treats directories as special type of files; they are indexed in the inode table.</li> <li>For each file or directory in a given directory, there is a string and a number in the data block(s) of the directory</li> <li>For each string, there may also be a length (assuming variable-sized names)</li> </ul> <p>Note</p> <p>in modern file systems, multiple blocks, lets say 8, are allocated (searched and marked for allocation) instead of 1 to store a file. This ensures that the files are stored contiguously.</p>"},{"location":"20-persistence/#file-reading","title":"File Reading","text":"<p>Note</p> <p>Amount of I/O generated by the open is proportional to the length of the pathname.</p> <p>For each additional directory in the path, we have to read its inode as well as its data. This can be made worse if the directory is large.</p>"},{"location":"20-persistence/#writing-data","title":"Writing Data","text":"<ul> <li>Each write to a file logically generates five I/Os:<ul> <li>one to read the data bitmap (which is then updated to mark the newly-allocated block as used)</li> <li>one to write the bitmap (to reflect its new state to disk)</li> <li>two more to read and then write the inode (which is updated with the new block\u2019s location),</li> <li>one to write the actual block itself.</li> </ul> </li> <li>See Book for detailed walkthrough</li> <li>Write operations generate a large amount of I/O calls.</li> </ul>"},{"location":"20-persistence/#caching-and-buffering","title":"Caching and Buffering","text":"<ul> <li>Every file open would require at least two reads for every level in the directory hierarchy (one to read the inode of the directory in question, and at least one to read its data)</li> <li>Most file systems aggressively use system memory to cache important blocks</li> </ul>"},{"location":"20-persistence/#static-partitioning","title":"Static Partitioning","text":"<ul> <li>Fixed size cache based on LRU policy</li> <li>Caches popular blocks of disk.</li> <li>Initialized at boot time and is generally 10% of the the total memory.</li> <li>Problem:<ul> <li>Wasteful - If the disk does not require 10% of the memory, the allocated pages go unused.</li> </ul> </li> </ul>"},{"location":"20-persistence/#dynamic-partitioning","title":"Dynamic Partitioning","text":"<ul> <li>Modern operating systems integrate virtual memory pages and file system pages into a unified page cache</li> <li>Memory can be allocated more flexibly across virtual memory and file system, depending on which needs more memory at a given time</li> </ul> <p>Note</p> <p>Reads can be optimized by a large amount of cache but write requests need to generate I/O operations to be persistent.</p>"},{"location":"20-persistence/#write-buffering","title":"Write Buffering","text":"<ul> <li>Delaying Writes<ul> <li>the file system can batch some updates into a smaller set of I/Os; for example, if an inode bitmap is updated when one file is created and then updated moments later as another file is created, the file system saves an I/O by delaying the write after the first update.</li> <li>Some writes can be avoided all together, in case if an application created a file and then deletes it.</li> </ul> </li> <li>By keeping writes in memory longer, performance can be improved by batching, scheduling, and even avoiding writes</li> <li>Problem:<ul> <li>if the system crashes before the updates have been propagated to disk, the updates are lost</li> </ul> </li> </ul>"},{"location":"30-concurrency/","title":"Concurrency","text":"<ul> <li>Threads and Concurrency</li> </ul>"},{"location":"30-concurrency/#multi-threaded-program","title":"Multi-Threaded Program","text":"<ul> <li>Program with more than one point of execution \u2192 Multiple Program Counters</li> <li>Threaded programs share the same memory space.</li> </ul>"},{"location":"30-concurrency/#benefits-of-threads","title":"Benefits of Threads","text":"<ul> <li>Parallelization<ul> <li>Process data in\u00a0parallel.</li> <li>If a computer program or system is parallelized, it breaks a problem down into smaller pieces to be independently solved simultaneously by discrete computing resources.</li> </ul> </li> <li>Specialization<ul> <li>Threads will get the data they need from the cache. (As other threads performing some similar workload might have put it there)</li> </ul> </li> <li>Time for context switch in threads is less, since memory is shared, hence mapping is not required between virtual and physical memory.</li> </ul>"},{"location":"30-concurrency/#are-threads-useful-on-a-single-cpu","title":"Are threads useful on a single CPU?","text":"<ul> <li>Yes, If the time for switching to Thread2 and back is less than the time, Thread1 stays idle (in an IO)</li> </ul>"},{"location":"30-concurrency/#are-threads-useful-when-threads-cpus","title":"Are threads useful when #threads &gt; #CPUs?","text":"<ul> <li>Yes, because cost of thread switching is less than the cost of process switching, as we threads share address space and thus no need to remap address space.</li> </ul>"},{"location":"30-concurrency/#multithreading-models","title":"Multithreading Models","text":"<ul> <li>User Threads: Supported above the kernel and are managed without any kernel support. (dummy threads) (cheaper to make)</li> <li>Kernel Threads: Supported and managed directly by the Operating System. (true threads) (expensive to make)</li> </ul>"},{"location":"30-concurrency/#many-to-one","title":"Many to One","text":"<ul> <li>Many user level threads mapped to one kernel thread.</li> <li>Pros:<ul> <li>Portable</li> <li>Not effected by OS limits on kernel threads and other policies.</li> </ul> </li> <li>Cons:<ul> <li>True parallelism is not possible as one kernel thread will process one user thread at a time. (even on multicore system)</li> <li>If a user thread makes a blocking call, the whole process blocks.</li> </ul> </li> </ul>"},{"location":"30-concurrency/#one-to-one","title":"One to One","text":"<ul> <li>Every user thread is mapped to a unique kernel thread.</li> <li>Pros:<ul> <li>Does not block the process if one thread makes a blocking call.</li> <li>Parallelism is possible.</li> </ul> </li> <li>Cons:<ul> <li>Spawning a kernel thread for every user thread is expensive.</li> <li>Dependence on the OS for all operations</li> <li>Affected by OS limitation on kernel threads and policies.</li> </ul> </li> </ul>"},{"location":"30-concurrency/#many-to-many","title":"Many to Many","text":"<ul> <li>Many user threads are mapped to smaller or equal number of kernel threads.</li> <li>Best of both worlds</li> </ul>"},{"location":"30-concurrency/#multithreading-patterns","title":"Multithreading Patterns","text":"<ul> <li>How to structure multithreaded applications?</li> </ul>"},{"location":"30-concurrency/#boss-worker","title":"Boss-Worker","text":"<ul> <li>A Single Boss assigns work.</li> <li>Many workers perform the work assigned by the boss.</li> <li>Throughput of the system is dependent on the boss.</li> <li>Boss and workers communicate by<ul> <li>directly signaling individual workers</li> <li>using a producer-consumer queue</li> </ul> </li> <li>Pros:<ul> <li>Simplicity</li> </ul> </li> <li>Cons:<ul> <li>thread pool needs to be managed.</li> <li>process-consumer queue needs to be managed</li> <li>Boss doesnt have notion of locality; a worker does better if its assigned similar tasks (cache), but for that boss will need to know the state of the worker, which is not supported by boss-worker model.</li> </ul> </li> </ul>"},{"location":"30-concurrency/#variants","title":"Variants","text":"<ul> <li>Workers can be specialized for certain tasks.</li> <li>Boss will now need to know what task to assign to what worker.</li> <li>Pros:<ul> <li>Better locality</li> </ul> </li> <li>Cons:<ul> <li>Load Balancing</li> </ul> </li> </ul>"},{"location":"30-concurrency/#pipeline","title":"Pipeline","text":"<ul> <li>Divide the main task into subtasks.</li> <li>Assign each worker a specific subtask.</li> <li>Throughput of the system depends on the subtask that takes the longest (throughput can be managed by increasing or decreasing workers per subtasks)</li> <li>One worker will depend on the other.</li> <li>Pros:<ul> <li>Specialization</li> <li>Locality</li> </ul> </li> <li>Cons:<ul> <li>Balancing</li> <li>Synchronization overheads</li> </ul> </li> </ul>"},{"location":"30-concurrency/#layered","title":"Layered","text":"<ul> <li>CHECK SLIDES</li> </ul>"},{"location":"30-concurrency/#locks","title":"Locks","text":"<ul> <li>Keep code atomic.</li> <li>Keep critical areas safe.</li> <li>CHECK SLIDES + Book</li> </ul>"},{"location":"30-concurrency/#semaphores","title":"Semaphores","text":"<ul> <li>PENDING</li> </ul>"},{"location":"30-concurrency/#multiprocessors","title":"Multiprocessors","text":"<ul> <li>Computer architects have had a difficult time making a single CPU much faster without using (way) too much power</li> <li>Multiple processes can be scheduled on different CPUs</li> <li>Multithreaded applications can spread work across multiple CPUs and thus run faster</li> </ul>"},{"location":"30-concurrency/#caches-vs-memory","title":"Caches vs Memory","text":"<ul> <li>The difference between the two is mainly around the use of hardware caches and how data is shared between the two.</li> </ul>"},{"location":"30-concurrency/#caches-and-multiple-cpus","title":"Caches and Multiple CPUs","text":"<ul> <li>In multi CPU architectures, caches can be shared.</li> <li>If there are three levels of caches, L0 and L1 will be dedicated to the CPU but L2 will be shared.</li> </ul>"},{"location":"30-concurrency/#cache-coherence","title":"Cache Coherence","text":"<ul> <li>Program running on one CPU can read the cache and write the cache the data, but as the CPU buffers the writes to main memory, if the program gets moved to another CPU with a different cache. It may not be able to access the updated data that is still sitting in the cache of the old CPU waiting to be written.</li> <li>Similarly if a program is running on the 2 CPUs with a certain variable if cached in the both CPUs, then if a CPU writes and mutates the value of the variable, the cache of the old CPU will not be updated and will cause issues.</li> <li>Generally, its is difficult to ensure that the data in caches is coherent across CPUs.</li> </ul>"},{"location":"30-concurrency/#hardware-cache-coherence","title":"Hardware - Cache Coherence","text":""},{"location":"30-concurrency/#write-invalidate-write-through","title":"Write Invalidate (write-through)","text":"<ul> <li>If one CPUs tries to modify a variable in the shared memory, hardware controller will invalidate the caches of the other CPUs.</li> <li>Subsequent reads will be issue to the Main memory.</li> <li>This only solves the problem if the caches being used are write-through (i.e. values are updated in both the main memory and the cache)</li> <li> <ul> <li>Lower Bandwidth - if you write to one cache, the only thing that happens is invalidation of other caches.</li> </ul> </li> </ul>"},{"location":"30-concurrency/#write-update-write-back","title":"Write Update (write-back)","text":"<ul> <li>If a cache is modified, all other caches are also modified with the updated value.</li> <li> <ul> <li>Large amount of traffic: needs to update all pages.</li> </ul> </li> <li> <ul> <li>The updated variable is immediately available.</li> </ul> </li> </ul>"},{"location":"30-concurrency/#atomic-instructions","title":"Atomic Instructions","text":"<ul> <li>Atomic instruction have to deal with concurrent execution in case of multiple CPUs.</li> <li>Atomic instructions are handed by a memory controller<ul> <li>which maintains the order and synchronization of the instructions.</li> <li>no 2 atomic instruction will get issued to the main memory directly.</li> </ul> </li> <li> <ul> <li>Takes Longer</li> </ul> </li> <li> <ul> <li>Generated coherence traffic</li> </ul> </li> </ul>"},{"location":"30-concurrency/#numa-aware-scheduling","title":"NUMA Aware Scheduling","text":"<ul> <li>In a multiprocessor architecture, there are multiple CPUs with multiple memory chips.</li> <li>The CPU closest to the memory containing the data will perform the fastest.</li> <li>When scheduling, ensure that the cpu closest to the memory with the data is scheduled to run.</li> <li>Non Uniform Memory Access NUMA aware scheduling - algorithms that take in account the location of the memory with respect to the CPU.</li> </ul>"},{"location":"30-concurrency/#cache-affinity-scheduling","title":"Cache Affinity &amp; Scheduling","text":"<ul> <li>Schedule threads onto the same CPU as much as possible to take advantage of hot caches.</li> </ul>"},{"location":"30-concurrency/#hyperthreading","title":"Hyperthreading","text":"<ul> <li>Multithreading on a single CPU</li> <li>Keep an array of registers with data of processes (only on of them is active)</li> <li>Context Switching is faster</li> <li>Challenge: What kind of threads should be co-scheduled:<ul> <li>Mix of I/O and CPU bound processes</li> </ul> </li> <li>Hyperthreading also reduces memory access latency by employing hot caches.</li> </ul>"},{"location":"30-concurrency/#scheduling-algorithms","title":"Scheduling Algorithms","text":""},{"location":"30-concurrency/#single-queue-multiprocessor-sqms","title":"Single Queue Multiprocessor - SQMS","text":"<ul> <li>Put all jobs that need to be scheduled into a single queue.</li> </ul>"},{"location":"30-concurrency/#advantage","title":"Advantage","text":"<ul> <li>Simplicity</li> </ul>"},{"location":"30-concurrency/#disadvantage","title":"Disadvantage","text":"<ul> <li>Locks on a single queue slows down execution</li> <li>Cache affinity is violated. Jobs end up bouncing around from CPU to CPU</li> <li>Some threads can be bound to specific CPUs and the rest of the threads can be allowed to bounce around.</li> </ul>"},{"location":"30-concurrency/#multi-queue-multiprocessor-scheduling-mqms","title":"Multi-Queue Multiprocessor Scheduling - MQMS","text":"<ul> <li>Multiple scheduling queues, one per processor/core</li> <li>When a job enters the system, it is placed on exactly one scheduling queue random, or picking one with fewer jobs than others<ul> <li>The job stays bound to the queue throughout its lifecycle</li> </ul> </li> </ul>"},{"location":"30-concurrency/#advantages","title":"Advantages","text":"<ul> <li>Cache Affinity</li> </ul>"},{"location":"30-concurrency/#disadvantages","title":"Disadvantages","text":""},{"location":"30-concurrency/#dealing-with-load-imbalance","title":"Dealing with Load Imbalance","text":""},{"location":"30-concurrency/#work-stealing","title":"Work stealing","text":"<ul> <li>A (source) queue that is low on jobs will occasionally peek at another (target) queue</li> <li>If the target queue is fuller than the source queue, the source will \u201csteal\u201d one or more jobs from the target to help balance load</li> <li>If you look around at other queues too often, you will suffer from high overhead and have trouble scaling</li> <li>If you don\u2019t look at other queues very often, you are in danger of suffering from severe load imbalances</li> <li>Finding the right threshold \u2192 a voodoo parameter</li> </ul>"}]}